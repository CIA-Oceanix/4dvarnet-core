/users/local/m19beauc/miniconda3/envs/4dvarnet/lib/python3.9/site-packages/hydra/_internal/hydra.py:98: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/upgrades/1.1_to_1.2/changes_to_job_working_dir for more information.
  ret = run_job(
Global seed set to 0
file_paths:
  obs_path: /users/local/m19beauc/SPDE_dc/python/SPDE_diffusion_dataset.nc
  gt_path: /users/local/m19beauc/SPDE_dc/python/SPDE_diffusion_dataset.nc
  spde_params_path: /users/local/m19beauc/SPDE_dc/python/SPDE_diffusion_dataset.nc
entrypoint:
  _target_: hydra_main.FourDVarNetHydraRunner.run
  max_epochs: 1
  progress_bar_refresh_rate: 5
params:
  files_cfg:
    obs_path: ${file_paths.obs_path}
    obs_var: 'y'
    gt_path: ${file_paths.gt_path}
    gt_var: x
    spde_params_path: ${file_paths.spde_params_path}
  iter_update:
  - 0
  - 20
  - 40
  - 60
  - 100
  - 150
  - 800
  nb_grad_update:
  - 20
  - 20
  - 20
  - 20
  - 20
  - 20
  - 50
  - 50
  - 50
  lr_update:
  - 0.001
  - 0.0001
  - 0.001
  - 0.0001
  - 0.0001
  - 1.0e-05
  - 1.0e-05
  - 1.0e-06
  - 1.0e-07
  k_batch: 1
  n_grad: 5
  dT: 5
  dx: 1
  W: 100
  resize_factor: 1
  shapeData:
  - 5
  - 100
  - 100
  dW: 3
  dW2: 1
  sS: 4
  nbBlocks: 1
  Nbpatches: 1
  stochastic: false
  size_ensemble: 1
  animate: true
  supervised: true
  estim_parameters: false
  DimAE: 25
  dim_grad_solver: 70
  dropout: 0.25
  dropout_phi_r: 0.0
  alpha_proj: 0.5
  alpha_sr: 0.5
  alpha_lr: 0.5
  alpha_mse_ssh: 10.0
  alpha_mse_gssh: 1.0
  sigNoise: 0.0
  flagSWOTData: true
  rnd1: 0
  rnd2: 100
  dwscale: 1
  UsePriodicBoundary: false
  InterpFlag: false
  automatic_optimization: true
  ckpt_name: modelGP_{epoch:02d}-{val_loss:.2f}
  norm_obs: l2
  norm_prior: l2
  diff_only: true
  test_domain:
    lat:
      _target_: builtins.slice
      _args_:
      - 0
      - 100
    lon:
      _target_: builtins.slice
      _args_:
      - 0
      - 100
datamodule:
  dim_range:
    lat:
      _target_: builtins.slice
      _args_:
      - 0
      - 100
    lon:
      _target_: builtins.slice
      _args_:
      - 0
      - 100
  _target_: oi.dataloading_gp.FourDVarNetDataModule
  slice_win:
    lat: 100
    lon: 100
    time: 5
  strides:
    lat: 100
    lon: 100
    time: 1
  train_slices:
  - _target_: builtins.slice
    _args_:
    - '2012-10-01'
    - '2012-11-20'
  - _target_: builtins.slice
    _args_:
    - '2013-02-07'
    - '2013-09-10'
  test_slices:
  - _target_: builtins.slice
    _args_:
    - '2013-01-03'
    - '2013-01-27'
  val_slices:
  - _target_: builtins.slice
    _args_:
    - '2012-11-30'
    - '2012-12-24'
  obs_path: ${params.files_cfg.obs_path}
  obs_var: ${params.files_cfg.obs_var}
  gt_path: ${params.files_cfg.gt_path}
  gt_var: ${params.files_cfg.gt_var}
  dl_kwargs:
    batch_size: 2
    num_workers: 2
  resize_factor: 1
lit_mod_cls: oi.lit_model_oi_ref_gd.LitModel
xp_name: oi_gp_ref_gd
seed: 0

file_paths:
  obs_path: /users/local/m19beauc/SPDE_dc/python/SPDE_diffusion_dataset.nc
  gt_path: /users/local/m19beauc/SPDE_dc/python/SPDE_diffusion_dataset.nc
  spde_params_path: /users/local/m19beauc/SPDE_dc/python/SPDE_diffusion_dataset.nc
entrypoint:
  _target_: hydra_main.FourDVarNetHydraRunner.run
  max_epochs: 1
  progress_bar_refresh_rate: 5
params:
  files_cfg:
    obs_path: ${file_paths.obs_path}
    obs_var: 'y'
    gt_path: ${file_paths.gt_path}
    gt_var: x
    spde_params_path: ${file_paths.spde_params_path}
  iter_update:
  - 0
  - 20
  - 40
  - 60
  - 100
  - 150
  - 800
  nb_grad_update:
  - 20
  - 20
  - 20
  - 20
  - 20
  - 20
  - 50
  - 50
  - 50
  lr_update:
  - 0.001
  - 0.0001
  - 0.001
  - 0.0001
  - 0.0001
  - 1.0e-05
  - 1.0e-05
  - 1.0e-06
  - 1.0e-07
  k_batch: 1
  n_grad: 5
  dT: 5
  dx: 1
  W: 100
  resize_factor: 1
  shapeData:
  - 5
  - 100
  - 100
  dW: 3
  dW2: 1
  sS: 4
  nbBlocks: 1
  Nbpatches: 1
  stochastic: false
  size_ensemble: 1
  animate: true
  supervised: true
  estim_parameters: false
  DimAE: 25
  dim_grad_solver: 70
  dropout: 0.25
  dropout_phi_r: 0.0
  alpha_proj: 0.5
  alpha_sr: 0.5
  alpha_lr: 0.5
  alpha_mse_ssh: 10.0
  alpha_mse_gssh: 1.0
  sigNoise: 0.0
  flagSWOTData: true
  rnd1: 0
  rnd2: 100
  dwscale: 1
  UsePriodicBoundary: false
  InterpFlag: false
  automatic_optimization: true
  ckpt_name: modelGP_{epoch:02d}-{val_loss:.2f}
  norm_obs: l2
  norm_prior: l2
  diff_only: true
  test_domain:
    lat:
      _target_: builtins.slice
      _args_:
      - 0
      - 100
    lon:
      _target_: builtins.slice
      _args_:
      - 0
      - 100
datamodule:
  dim_range:
    lat:
      _target_: builtins.slice
      _args_:
      - 0
      - 100
    lon:
      _target_: builtins.slice
      _args_:
      - 0
      - 100
  _target_: oi.dataloading_gp.FourDVarNetDataModule
  slice_win:
    lat: 100
    lon: 100
    time: 5
  strides:
    lat: 100
    lon: 100
    time: 1
  train_slices:
  - _target_: builtins.slice
    _args_:
    - '2012-10-01'
    - '2012-11-20'
  - _target_: builtins.slice
    _args_:
    - '2013-02-07'
    - '2013-09-10'
  test_slices:
  - _target_: builtins.slice
    _args_:
    - '2013-01-03'
    - '2013-01-27'
  val_slices:
  - _target_: builtins.slice
    _args_:
    - '2012-11-30'
    - '2012-12-24'
  obs_path: ${params.files_cfg.obs_path}
  obs_var: ${params.files_cfg.obs_var}
  gt_path: ${params.files_cfg.gt_path}
  gt_var: ${params.files_cfg.gt_var}
  dl_kwargs:
    batch_size: 2
    num_workers: 2
  resize_factor: 1
lit_mod_cls: oi.lit_model_oi_ref_gd.LitModel
xp_name: oi_gp_ref_gd
seed: 0

Warning: ecCodes 2.21.0 or higher is recommended. You are running version 2.16.0
Using current oi.metrics
[2022-09-19 16:04:34,251][cartopy_userconfig][INFO] - Setting cartopy.config["pre_existing_data_dir"] to /users/local/m19beauc/miniconda3/envs/4dvarnet/share/cartopy. Don't worry, this is probably intended behaviour to avoid failing downloads of geological data behind a firewall.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type            | Params
-------------------------------------------------
0 | model        | Gradient_Solver | 49.1 K
1 | model_LR     | ModelLR         | 0     
2 | gradient_img | Gradient_img    | 18    
-------------------------------------------------
49.1 K    Trainable params
23        Non-trainable params
49.1 K    Total params
0.196     Total estimated model params size (MB)
{'lat': 100, 'lon': 100, 'time': 5}
get_model:  None
{'max_epochs': 1, 'progress_bar_refresh_rate': 5}
{'num_nodes': 1, 'gpus': 1, 'logger': True, 'auto_select_gpus': True, 'callbacks': [<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7efeb485c5b0>, <pytorch_lightning.callbacks.lr_monitor.LearningRateMonitor object at 0x7efeb485c2e0>], 'max_epochs': 1, 'progress_bar_refresh_rate': 5}
Validation sanity check: 0it [00:00, ?it/s]/users/local/m19beauc/miniconda3/envs/4dvarnet/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]                                                              Global seed set to 0
/users/local/m19beauc/miniconda3/envs/4dvarnet/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/users/local/m19beauc/miniconda3/envs/4dvarnet/lib/python3.9/site-packages/pytorch_lightning/callbacks/lr_monitor.py:112: RuntimeWarning: You are using `LearningRateMonitor` callback with models that have no learning rate schedulers. Please see documentation for `configure_optimizers` method.
  rank_zero_warn(
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/141 [00:00<00:00, 4934.48it/s]Epoch 0:   0%|          | 0/141 [00:00<00:00, 2406.37it/s] /users/local/m19beauc/miniconda3/envs/4dvarnet/lib/python3.9/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:558: UserWarning: training_step returned None. If this was on purpose, ignore this warning...
  self._warning_cache.warn(
Epoch 0:   4%|â–Ž         | 5/141 [00:00<00:07, 17.51it/s]  Epoch 0:   4%|â–Ž         | 5/141 [00:00<00:07, 17.49it/s, loss=nan, v_num=10]Epoch 0:   7%|â–‹         | 10/141 [00:00<00:04, 30.33it/s, loss=nan, v_num=10]Epoch 0:  11%|â–ˆ         | 15/141 [00:00<00:03, 41.27it/s, loss=nan, v_num=10]Epoch 0:  14%|â–ˆâ–        | 20/141 [00:00<00:02, 49.41it/s, loss=nan, v_num=10]Epoch 0:  18%|â–ˆâ–Š        | 25/141 [00:00<00:02, 57.54it/s, loss=nan, v_num=10]Epoch 0:  18%|â–ˆâ–Š        | 25/141 [00:00<00:02, 57.50it/s, loss=nan, v_num=10]Epoch 0:  21%|â–ˆâ–ˆâ–       | 30/141 [00:00<00:01, 62.83it/s, loss=nan, v_num=10]Epoch 0:  25%|â–ˆâ–ˆâ–       | 35/141 [00:00<00:01, 69.49it/s, loss=nan, v_num=10]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 40/141 [00:00<00:01, 75.52it/s, loss=nan, v_num=10]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 45/141 [00:00<00:01, 81.52it/s, loss=nan, v_num=10]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 45/141 [00:00<00:01, 81.48it/s, loss=nan, v_num=10]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 50/141 [00:00<00:01, 85.96it/s, loss=nan, v_num=10]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 55/141 [00:00<00:00, 89.95it/s, loss=nan, v_num=10]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 60/141 [00:00<00:00, 94.14it/s, loss=nan, v_num=10]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 65/141 [00:00<00:00, 97.41it/s, loss=nan, v_num=10]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 65/141 [00:00<00:00, 97.36it/s, loss=nan, v_num=10]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 70/141 [00:00<00:00, 100.55it/s, loss=nan, v_num=10]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 75/141 [00:00<00:00, 102.73it/s, loss=nan, v_num=10]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 80/141 [00:00<00:00, 104.93it/s, loss=nan, v_num=10]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 85/141 [00:00<00:00, 108.08it/s, loss=nan, v_num=10]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 85/141 [00:00<00:00, 108.01it/s, loss=nan, v_num=10]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 90/141 [00:00<00:00, 110.85it/s, loss=nan, v_num=10]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 95/141 [00:00<00:00, 112.85it/s, loss=nan, v_num=10]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 100/141 [00:00<00:00, 114.66it/s, loss=nan, v_num=10]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 105/141 [00:00<00:00, 116.55it/s, loss=nan, v_num=10]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 105/141 [00:00<00:00, 116.52it/s, loss=nan, v_num=10]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 110/141 [00:00<00:00, 118.48it/s, loss=nan, v_num=10]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 115/141 [00:01<00:00, 115.99it/s, loss=nan, v_num=10]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 120/141 [00:01<00:00, 117.92it/s, loss=nan, v_num=10]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 125/141 [00:01<00:00, 118.88it/s, loss=nan, v_num=10]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 125/141 [00:01<00:00, 118.82it/s, loss=nan, v_num=10]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 130/141 [00:01<00:00, 110.73it/s, loss=nan, v_num=10]
Validating: 0it [00:00, ?it/s][A
Validating:   0%|          | 0/11 [00:00<?, ?it/s][A
Validating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 5/11 [00:00<00:00, 22.71it/s][AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 87.91it/s, loss=nan, v_num=10] 
                                                          [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 87.80it/s, loss=nan, v_num=10]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]
/users/local/m19beauc/miniconda3/envs/4dvarnet/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(

Testing: 0it [00:00, ?it/s]